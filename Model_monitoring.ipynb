{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcbc543c-5978-4eeb-859a-86870abe7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580256a3-64a9-4d47-94bf-24a82459122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"inputs/Predicted_train.csv\")\n",
    "test = pd.read_csv(\"inputs/Predicted_test.csv\")\n",
    "val = pd.read_csv(\"inputs/Predicted_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d904c-3aef-41a4-abc4-5a30fe7647c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc9bdb-85f7-4d63-871e-01e74958490e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "815b854f-b318-4b91-9b4c-e9bb0e8331c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check model drift\n",
    "def check_model_drift(ref_metric_dict,cur_metric_dict,type='classification',tol=0.1):\n",
    "    if type == 'classification':\n",
    "        precision_change = abs((cur_metric_dict['Precision']-ref_metric_dict['Precision'])/ref_metric_dict['Precision'])\n",
    "        recall_change = abs((cur_metric_dict['Recall']-ref_metric_dict['Recall'])/ref_metric_dict['Recall'])\n",
    "        roc_auc_change = abs((cur_metric_dict['Roc-Auc']-ref_metric_dict['Roc-Auc'])/ref_metric_dict['Roc-Auc'])\n",
    "\n",
    "        counter = 0\n",
    "        for i in [precision_change,recall_change,roc_auc_change]:\n",
    "            if i > tol:\n",
    "                counter += 1\n",
    "\n",
    "        if counter > 0:\n",
    "            print(\"ALERT! There is a model drift.\")\n",
    "            print(\"Change in Precision: \"+ str(np.round(100*precision_change,2))+\"%\")\n",
    "            print(\"Change in Recall: \"+ str(np.round(100*recall_change,2))+\"%\")\n",
    "            print(\"Change in Roc-Auc: \"+ str(np.round(100*roc_auc_change,2))+\"%\")\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"There is no model drift.\")\n",
    "            return 0\n",
    "\n",
    "    elif type == 'regression':\n",
    "        r2_change = abs((cur_metric_dict['R2_score']-ref_metric_dict['R2_score'])/ref_metric_dict['R2_score'])\n",
    "        rmse_change = abs((cur_metric_dict['RMSE']-ref_metric_dict['RMSE'])/ref_metric_dict['RMSE'])\n",
    "        mae_change = abs((cur_metric_dict['MAE']-ref_metric_dict['MAE'])/ref_metric_dict['MAE'])\n",
    "        \n",
    "        counter = 0\n",
    "        for i in [rmse_change,mae_change, r2_change]:\n",
    "            if i > tol:\n",
    "                counter += 1\n",
    "\n",
    "        if counter > 0:\n",
    "            print(\"ALERT! There is a model drift.\")\n",
    "            R2_CHANGE = np.round(r2_change, 2)\n",
    "            RMSE_CHANGE = np.round(100*rmse_change,2)\n",
    "            MAE_CHANGE = np.round(100*mae_change,2)\n",
    "            print(\"Change in R2 Score: \"+ str(np.round(r2_change,2)))\n",
    "            print(\"Change in RMSE: \"+ str(np.round(100*rmse_change,2))+\"%\")\n",
    "            print(\"Change in MAE: \"+ str(np.round(100*mae_change,2))+\"%\")\n",
    "            return 1, RMSE_CHANGE, MAE_CHANGE, R2_CHANGE\n",
    "        else:\n",
    "            print(\"There is no model drift.\")\n",
    "            RMSE_CHANGE = 'NONE'\n",
    "            MAE_CHANGE = 'NONE'\n",
    "            return 0, RMSE_CHANGE, MAE_CHANGE\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77d7fc7-b902-4d07-aff5-76f329f9cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_model(new_perform_dict, old_perform_dict):\n",
    "    count = 0\n",
    "    for metric in new_perform_dict.keys():\n",
    "        if new_perform_dict[metric] < old_perform_dict[metric]:\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        return 'New Model'\n",
    "    else:\n",
    "        return 'Old Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "def91409-cd34-43fc-b6f6-b6f0a6d33552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_monitoring(test):\n",
    "    actual = test['PRICE_IN_LAKHS']\n",
    "    predicted = test['PREDICTED_PRICE_IN_LAKHS']\n",
    "\n",
    "    r2score = r2_score(actual,predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual,predicted))\n",
    "    mae = np.sqrt(mean_absolute_error(actual,predicted))\n",
    "#     print(\"RMSE: \", rmse)\n",
    "#     print(\"MAE: \", mae)\n",
    "\n",
    "    scoring_ref_metrics = {}\n",
    "    scoring_ref_metrics['R2_score'] = r2score\n",
    "    scoring_ref_metrics['RMSE'] = rmse\n",
    "    scoring_ref_metrics['MAE'] = mae #+ 0.2*mae\n",
    "#     print(scoring_ref_metrics)\n",
    "    \n",
    "    \n",
    "    # Loading the reference performance dict (from training)\n",
    "    with open('model/MODEL_XGB_PERFM_METRICS.pkl', 'rb') as F:\n",
    "        model_ref_metric = pickle.load(F)\n",
    "        \n",
    "#     print(model_ref_metric)\n",
    "    \n",
    "    # Check for model drift\n",
    "    model_drift, RMSE_CHANGE, MAE_CHANGE = check_model_drift(model_ref_metric,scoring_ref_metrics,type='regression',tol=0.1)\n",
    "    \n",
    "    # Log values\n",
    "    log = {}\n",
    "    #log['Time Period'] = str(batch_df['ADMISSION_DATE'].min()) + ' to ' + str(batch_df['ADMISSION_DATE'].max())\n",
    "    #log['Total Records'] = batch_df.shape[0]\n",
    "    log['Scoring Metrics'] = scoring_ref_metrics\n",
    "    log['Training Metrics'] = model_ref_metric\n",
    "    log['Model Drift IND'] = model_drift\n",
    "    log['RMSE Change'] = RMSE_CHANGE\n",
    "    log['MAE Change'] = MAE_CHANGE\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9200c935-8ee0-4417-a242-062860f6e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT! There is a model drift.\n",
      "Change in R2 Score: 0.0\n",
      "Change in RMSE: 0.0%\n",
      "Change in MAE: 65.14%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Scoring Metrics': {'R2_score': 0.9226522226658003,\n",
       "  'RMSE': 10.84660054827755,\n",
       "  'MAE': 2.8685646151777857},\n",
       " 'Training Metrics': {'R2_score': 0.9226522226658003,\n",
       "  'RMSE': 10.84660054827755,\n",
       "  'MAE': 8.228662951450076},\n",
       " 'Model Drift IND': 1,\n",
       " 'RMSE Change': 0.0,\n",
       " 'MAE Change': 65.14}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = model_monitoring(test)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7bd51-323c-41ce-92d8-4686968f60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging the training performance metrics and the trained model\n",
    "precision = metrics.precision_score(y_train_encode,y_pred_class_encode)\n",
    "recall = metrics.recall_score(y_train_encode,y_pred_class_encode)\n",
    "roc_auc = metrics.roc_auc_score(y_train_encode,y_pred)\n",
    "\n",
    "training_performance_metrics = dict()\n",
    "training_performance_metrics['Precision'] = np.round(precision,2)\n",
    "training_performance_metrics['Recall'] = np.round(recall,2)\n",
    "training_performance_metrics['Roc-Auc'] = np.round(roc_auc,2)\n",
    "\n",
    "print(training_performance_metrics)\n",
    "\n",
    "with open('Training_Perfrom_Metrics.pkl','wb') as F:\n",
    "    pickle.dump(training_performance_metrics,F)\n",
    "\n",
    "with open('RF_Loan_Model.pkl','wb') as F:\n",
    "    pickle.dump(rf,F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d0eb9-0c86-489f-9cce-5f7187dfea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_drift:\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "\n",
    "\n",
    "    def finalize_model(self, new_perform_dict, old_perform_dict):\n",
    "        count = 0\n",
    "        for metric in new_perform_dict.keys():\n",
    "            if new_perform_dict[metric] < old_perform_dict[metric]:\n",
    "                count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            return 'New Model'\n",
    "        else:\n",
    "            return 'Old Model'\n",
    "\n",
    "\n",
    "    # Function to check model drift\n",
    "    def check_model_drift(ref_metric_dict,cur_metric_dict,model_type='classification',tol=0.1):\n",
    "        if type == 'classification':\n",
    "            precision_change = abs((cur_metric_dict['Precision']-ref_metric_dict['Precision'])/ref_metric_dict['Precision'])\n",
    "            recall_change = abs((cur_metric_dict['Recall']-ref_metric_dict['Recall'])/ref_metric_dict['Recall'])\n",
    "            roc_auc_change = abs((cur_metric_dict['Roc-Auc']-ref_metric_dict['Roc-Auc'])/ref_metric_dict['Roc-Auc'])\n",
    "    \n",
    "            counter = 0\n",
    "            for i in [precision_change,recall_change,roc_auc_change]:\n",
    "                if i > tol:\n",
    "                    counter += 1\n",
    "    \n",
    "            if counter > 0:\n",
    "                print(\"ALERT! There is a model drift.\")\n",
    "                print(\"Change in Precision: \"+ str(np.round(100*precision_change,2))+\"%\")\n",
    "                print(\"Change in Recall: \"+ str(np.round(100*recall_change,2))+\"%\")\n",
    "                print(\"Change in Roc-Auc: \"+ str(np.round(100*roc_auc_change,2))+\"%\")\n",
    "                return 1\n",
    "            else:\n",
    "                print(\"There is no model drift.\")\n",
    "                return 0\n",
    "    \n",
    "        elif type == 'regression':\n",
    "            r2_change = abs((cur_metric_dict['R2_score']-ref_metric_dict['R2_score'])/ref_metric_dict['R2_score'])\n",
    "            rmse_change = abs((cur_metric_dict['RMSE']-ref_metric_dict['RMSE'])/ref_metric_dict['RMSE'])\n",
    "            mae_change = abs((cur_metric_dict['MAE']-ref_metric_dict['MAE'])/ref_metric_dict['MAE'])\n",
    "            \n",
    "            counter = 0\n",
    "            for i in [rmse_change,mae_change, r2_change]:\n",
    "                if i > tol:\n",
    "                    counter += 1\n",
    "    \n",
    "            if counter > 0:\n",
    "                print(\"ALERT! There is a model drift.\")\n",
    "                R2_CHANGE = np.round(r2_change, 2)\n",
    "                RMSE_CHANGE = np.round(100*rmse_change,2)\n",
    "                MAE_CHANGE = np.round(100*mae_change,2)\n",
    "                print(\"Change in R2 Score: \"+ str(np.round(r2_change,2)))\n",
    "                print(\"Change in RMSE: \"+ str(np.round(100*rmse_change,2))+\"%\")\n",
    "                print(\"Change in MAE: \"+ str(np.round(100*mae_change,2))+\"%\")\n",
    "                return 1, RMSE_CHANGE, MAE_CHANGE, R2_CHANGE\n",
    "            else:\n",
    "                print(\"There is no model drift.\")\n",
    "                RMSE_CHANGE = 'NONE'\n",
    "                MAE_CHANGE = 'NONE'\n",
    "                R2_CHANGE = 'NONE'\n",
    "                return 0, RMSE_CHANGE, MAE_CHANGE, R2_CHANGE\n",
    "\n",
    "\n",
    "\n",
    "        def retrain_model(self):\n",
    "            # Loading the scoring data\n",
    "            data = pd.DataFrame(pd.read_sql(retraining_batch_query(cut_off_date),conn))\n",
    "            data.columns = [col.upper() for col in data.columns.tolist()]\n",
    "            print(data.shape)\n",
    "            #display(data.head())\n",
    "    \n",
    "            # Splitting the data into Train and Test set\n",
    "            import pytz    \n",
    "            from datetime import datetime, timedelta\n",
    "            tz_NY = pytz.timezone('Asia/Kolkata')\n",
    "    \n",
    "            max_date = data.ADMISSION_DATE.max()\n",
    "            min_date = max_date - timedelta(days=7)\n",
    "    \n",
    "            data_train = data[(data['ADMISSION_DATE'] <= min_date)]\n",
    "            data_test = data[(data['ADMISSION_DATE'] >= min_date) & (data['ADMISSION_DATE'] <= max_date)]\n",
    "    \n",
    "    \n",
    "            # Applying the preprocessing steps\n",
    "            df_train_processed = LOS_Preprocessing.preprocess_data(data_train)\n",
    "            print(df_train_processed.shape)\n",
    "    \n",
    "            df_test_processed = LOS_Preprocessing.preprocess_data(data_test)\n",
    "            print(df_test_processed.shape)\n",
    "    \n",
    "            # Performing feature selection\n",
    "            df_final = df_train_processed.copy()\n",
    "            print(df_final.shape)\n",
    "        #     display(df_final.head())\n",
    "            print(\"Feature Selection Started..\")\n",
    "            model_feats = feature_selection(df_final)\n",
    "            print(model_feats)\n",
    "            model_feats.remove('LOS')\n",
    "    \n",
    "            # Model Building\n",
    "            import xgboost as xgb\n",
    "    \n",
    "            xgb_ = xgb.XGBRegressor()\n",
    "            xgb_.fit(df_final[model_feats],df_final['LOS'])\n",
    "    \n",
    "            df_test_final = check_n_create_model_features(df_test_processed,model_feats)\n",
    "            if 'LOS' in df_test_final.columns.tolist():\n",
    "                df_test_final = df_test_final.drop('LOS',axis=1)\n",
    "            preds = np.ceil(xgb_.predict(df_test_final))\n",
    "            rmse = np.sqrt(metrics.mean_squared_error(df_test_processed['LOS'],preds))\n",
    "            mae = np.sqrt(metrics.mean_absolute_error(df_test_processed['LOS'],preds))\n",
    "            print(\"\\n Test Performance (new model)\")\n",
    "            print(\"RMSE: \", rmse)\n",
    "            print(\"MAE: \", mae)      \n",
    "    \n",
    "            # Saving the trained model\n",
    "            booster = xgb_.get_booster()\n",
    "            booster.save_model('./Retraining Artifacts/MODEL_XGB.model')\n",
    "    \n",
    "            model_xgb_metrics_new = {}\n",
    "            model_xgb_metrics_new['RMSE'] = rmse\n",
    "            model_xgb_metrics_new['MAE'] = mae\n",
    "    \n",
    "            import pickle\n",
    "    \n",
    "            with open('./Retraining Artifacts/MODEL_XGB_PERFM_METRICS.pkl','wb') as F:\n",
    "                pickle.dump(model_xgb_metrics_new,F)\n",
    "    \n",
    "    \n",
    "            # Getting the predictions from the old model\n",
    "            model = xgboost.XGBRegressor()\n",
    "            model.load_model('MODEL_XGB.model')\n",
    "        #     df_test_processed['PREDICTED_LOS'] = np.ceil(model.predict(df_test_processed[model_feats]))\n",
    "    \n",
    "            with open('MODEL_FEATS.pkl','rb') as F:\n",
    "                model_feats_old = pickle.load(F)\n",
    "    \n",
    "            df_test_final = check_n_create_model_features(df_test_processed,model_feats_old)\n",
    "            if 'LOS' in df_test_final.columns.tolist():\n",
    "                df_test_final = df_test_final.drop('LOS',axis=1)\n",
    "            preds = np.ceil(model.predict(df_test_final))\n",
    "            rmse = np.sqrt(metrics.mean_squared_error(df_test_processed['LOS'],preds))\n",
    "            mae = np.sqrt(metrics.mean_absolute_error(df_test_processed['LOS'],preds))\n",
    "            print(\"\\n Test Performance (old model)\")\n",
    "            print(\"RMSE: \", rmse)\n",
    "            print(\"MAE: \", mae)   \n",
    "    \n",
    "            model_xgb_metrics_old = {}\n",
    "            model_xgb_metrics_old['RMSE'] = rmse\n",
    "            model_xgb_metrics_old['MAE'] = mae\n",
    "        \n",
    "        return model_xgb_metrics_new, model_xgb_metrics_old\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def deploy_model(self, selector='Old Model'):\n",
    "        if selector != 'Old Model':\n",
    "            # STEP-1:\n",
    "            # Loading the old model\n",
    "            with open('MODEL_FEATS.pkl','rb') as F:\n",
    "                old_feats = pickle.load(F)\n",
    "            with open('MODEL_XGB.model','rb') as F:\n",
    "                old_model = pickle.load(F)\n",
    "            with open('MODEL_XGB_PERFM_METRICS.pkl','rb') as F:\n",
    "                old_perfm_dict = pickle.load(F)\n",
    "                \n",
    "            # Saving the copy to Archive folder\n",
    "            with open('./Archive/MODEL_FEATS.pkl','wb') as F:\n",
    "                pickle.dump(old_feats,F)\n",
    "            with open('./Archive/MODEL_XGB.model','wb') as F:\n",
    "                pickle.dump(old_model,F)\n",
    "            with open('./Archive/MODEL_XGB_PERFM_METRICS.pkl','wb') as F:\n",
    "                pickle.dump(old_perfm_dict,F)\n",
    "                \n",
    "            # STEP-2:\n",
    "            # Loadin the new model\n",
    "            with open('./Retraining Artifacts/MODEL_FEATS.pkl','rb') as F:\n",
    "                new_feats = pickle.load(F)\n",
    "            with open('./Retraining Artifacts/MODEL_XGB.model','rb') as F:\n",
    "                new_model = pickle.load(F)\n",
    "            with open('./Retraining Artifacts/MODEL_XGB_PERFM_METRICS.pkl','rb') as F:\n",
    "                new_perfm_dict = pickle.load(F)\n",
    "                \n",
    "            # Replacing the old model artifacts with the new model\n",
    "            with open('MODEL_FEATS.pkl','wb') as F:\n",
    "                pickle.dump(new_feats,F)\n",
    "            with open('MODEL_XGB.model','wb') as F:\n",
    "                pickle.dump(new_model,F)\n",
    "            with open('MODEL_XGB_PERFM_METRICS.pkl','wb') as F:\n",
    "                pickle.dump(new_perfm_dict,F)\n",
    "                \n",
    "        return 'Deployment Successful'\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
