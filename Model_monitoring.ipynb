{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcbc543c-5978-4eeb-859a-86870abe7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580256a3-64a9-4d47-94bf-24a82459122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"inputs/Predicted_train.csv\")\n",
    "test = pd.read_csv(\"inputs/Predicted_test.csv\")\n",
    "val = pd.read_csv(\"inputs/Predicted_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d904c-3aef-41a4-abc4-5a30fe7647c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc9bdb-85f7-4d63-871e-01e74958490e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "815b854f-b318-4b91-9b4c-e9bb0e8331c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check model drift\n",
    "def check_model_drift(ref_metric_dict,cur_metric_dict,type='classification',tol=0.1):\n",
    "    if type == 'classification':\n",
    "        precision_change = abs((cur_metric_dict['Precision']-ref_metric_dict['Precision'])/ref_metric_dict['Precision'])\n",
    "        recall_change = abs((cur_metric_dict['Recall']-ref_metric_dict['Recall'])/ref_metric_dict['Recall'])\n",
    "        roc_auc_change = abs((cur_metric_dict['Roc-Auc']-ref_metric_dict['Roc-Auc'])/ref_metric_dict['Roc-Auc'])\n",
    "\n",
    "        counter = 0\n",
    "        for i in [precision_change,recall_change,roc_auc_change]:\n",
    "            if i > tol:\n",
    "                counter += 1\n",
    "\n",
    "        if counter > 0:\n",
    "            print(\"ALERT! There is a model drift.\")\n",
    "            print(\"Change in Precision: \"+ str(np.round(100*precision_change,2))+\"%\")\n",
    "            print(\"Change in Recall: \"+ str(np.round(100*recall_change,2))+\"%\")\n",
    "            print(\"Change in Roc-Auc: \"+ str(np.round(100*roc_auc_change,2))+\"%\")\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"There is no model drift.\")\n",
    "            return 0\n",
    "\n",
    "    elif type == 'regression':\n",
    "        r2_change = abs((cur_metric_dict['R2_score']-ref_metric_dict['R2_score'])/ref_metric_dict['R2_score'])\n",
    "        rmse_change = abs((cur_metric_dict['RMSE']-ref_metric_dict['RMSE'])/ref_metric_dict['RMSE'])\n",
    "        mae_change = abs((cur_metric_dict['MAE']-ref_metric_dict['MAE'])/ref_metric_dict['MAE'])\n",
    "        \n",
    "        counter = 0\n",
    "        for i in [rmse_change,mae_change]:\n",
    "            if i > tol:\n",
    "                counter += 1\n",
    "\n",
    "        if counter > 0:\n",
    "            print(\"ALERT! There is a model drift.\")\n",
    "            RMSE_CHANGE = np.round(100*rmse_change,2)\n",
    "            MAE_CHANGE = np.round(100*mae_change,2)\n",
    "            print(\"Change in R2 Score: \"+ str(np.round(rmse_change,2)))\n",
    "            print(\"Change in RMSE: \"+ str(np.round(100*rmse_change,2))+\"%\")\n",
    "            print(\"Change in MAE: \"+ str(np.round(100*mae_change,2))+\"%\")\n",
    "            return 1, RMSE_CHANGE, MAE_CHANGE\n",
    "        else:\n",
    "            print(\"There is no model drift.\")\n",
    "            RMSE_CHANGE = 'NONE'\n",
    "            MAE_CHANGE = 'NONE'\n",
    "            return 0, RMSE_CHANGE, MAE_CHANGE\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77d7fc7-b902-4d07-aff5-76f329f9cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_model(new_perform_dict, old_perform_dict):\n",
    "    count = 0\n",
    "    for metric in new_perform_dict.keys():\n",
    "        if new_perform_dict[metric] < old_perform_dict[metric]:\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        return 'New Model'\n",
    "    else:\n",
    "        return 'Old Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "def91409-cd34-43fc-b6f6-b6f0a6d33552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_monitoring(test):\n",
    "    actual = test['PRICE_IN_LAKHS']\n",
    "    predicted = test['PREDICTED_PRICE_IN_LAKHS']\n",
    "\n",
    "    r2score = r2_score(actual,predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual,predicted))\n",
    "    mae = np.sqrt(mean_absolute_error(actual,predicted))\n",
    "#     print(\"RMSE: \", rmse)\n",
    "#     print(\"MAE: \", mae)\n",
    "\n",
    "    scoring_ref_metrics = {}\n",
    "    scoring_ref_metrics['R2_score'] = r2score\n",
    "    scoring_ref_metrics['RMSE'] = rmse\n",
    "    scoring_ref_metrics['MAE'] = mae #+ 0.2*mae\n",
    "#     print(scoring_ref_metrics)\n",
    "    \n",
    "    \n",
    "    # Loading the reference performance dict (from training)\n",
    "    with open('model/MODEL_XGB_PERFM_METRICS.pkl', 'rb') as F:\n",
    "        model_ref_metric = pickle.load(F)\n",
    "        \n",
    "#     print(model_ref_metric)\n",
    "    \n",
    "    # Check for model drift\n",
    "    model_drift, RMSE_CHANGE, MAE_CHANGE = check_model_drift(model_ref_metric,scoring_ref_metrics,type='regression',tol=0.1)\n",
    "    \n",
    "    # Log values\n",
    "    log = {}\n",
    "    #log['Time Period'] = str(batch_df['ADMISSION_DATE'].min()) + ' to ' + str(batch_df['ADMISSION_DATE'].max())\n",
    "    #log['Total Records'] = batch_df.shape[0]\n",
    "    log['Scoring Metrics'] = scoring_ref_metrics\n",
    "    log['Training Metrics'] = model_ref_metric\n",
    "    log['Model Drift IND'] = model_drift\n",
    "    log['RMSE Change'] = RMSE_CHANGE\n",
    "    log['MAE Change'] = MAE_CHANGE\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9200c935-8ee0-4417-a242-062860f6e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT! There is a model drift.\n",
      "Change in R2 Score: 0.0\n",
      "Change in RMSE: 0.0%\n",
      "Change in MAE: 65.14%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Scoring Metrics': {'R2_score': 0.9226522226658003,\n",
       "  'RMSE': 10.84660054827755,\n",
       "  'MAE': 2.8685646151777857},\n",
       " 'Training Metrics': {'R2_score': 0.9226522226658003,\n",
       "  'RMSE': 10.84660054827755,\n",
       "  'MAE': 8.228662951450076},\n",
       " 'Model Drift IND': 1,\n",
       " 'RMSE Change': 0.0,\n",
       " 'MAE Change': 65.14}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = model_monitoring(test)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7bd51-323c-41ce-92d8-4686968f60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging the training performance metrics and the trained model\n",
    "precision = metrics.precision_score(y_train_encode,y_pred_class_encode)\n",
    "recall = metrics.recall_score(y_train_encode,y_pred_class_encode)\n",
    "roc_auc = metrics.roc_auc_score(y_train_encode,y_pred)\n",
    "\n",
    "training_performance_metrics = dict()\n",
    "training_performance_metrics['Precision'] = np.round(precision,2)\n",
    "training_performance_metrics['Recall'] = np.round(recall,2)\n",
    "training_performance_metrics['Roc-Auc'] = np.round(roc_auc,2)\n",
    "\n",
    "print(training_performance_metrics)\n",
    "\n",
    "with open('Training_Perfrom_Metrics.pkl','wb') as F:\n",
    "    pickle.dump(training_performance_metrics,F)\n",
    "\n",
    "with open('RF_Loan_Model.pkl','wb') as F:\n",
    "    pickle.dump(rf,F)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
